package com.github.r73pls.djl_Project.imageClassificftion;

import ai.djl.ndarray.NDArray;
import ai.djl.ndarray.NDList;
import ai.djl.ndarray.NDManager;
import ai.djl.ndarray.types.DataType;
import ai.djl.ndarray.types.Shape;

public class InizializeModelParametrs {
    /**
     * Как и в нашем примере с линейной регрессией, каждый пример здесь будет представлен вектором фиксированной длины.
     * Каждый пример в необработанных данных представляет собой изображение, умноженное на 28.
     * В этом разделе мы сгладим каждое изображение, рассматривая его как 1D-векторы длиной в 784 дюйма.
     * В будущем мы поговорим о более сложных стратегиях использования пространственной структуры изображений,
     * но пока мы рассматриваем расположение каждого пикселя как еще одну особенность.
     * в регрессии softmax у нас столько выходных данных, сколько существует категорий. Поскольку наш набор
     * данных содержит 10 категорий, наша сеть будет иметь выходное измерение 10. Следовательно,
     * наши веса будут составлять матрицу 784 умножить на 10, а смещения будут составлять вектор 1 умножить на 10.
     * Как и в случае с линейной регрессией, мы будем инициализировать наши веса W с помощью гауссовского шума
     * и наших смещений, чтобы принять начальное значение 0.
     */

    int numInputs = 784;
    int numOutputs = 10;

    NDManager manager = NDManager.newBaseManager();
    NDArray W = manager.randomNormal(0, 0.01f, new Shape(numInputs, numOutputs), DataType.FLOAT32);
    NDArray b = manager.zeros(new Shape(numOutputs), DataType.FLOAT32);
    NDList params = new NDList(W, b);
/**
 * Прежде чем внедрять регрессионную модель softmax, давайте кратко рассмотрим, как такие операторы, как sum(),
 * работают с определенными измерениями в NDArray. Учитывая матрицу X, мы можем суммировать по всем элементам
 * (по умолчанию) или только по элементам на одной оси, т.е. по столбцу (new int[]{0}) или по одной и той же
 * строке (new int[]{1}). Мы помещаем ось в массив int, поскольку мы также можем указать несколько осей.
 * Например, если мы вызываем функцию sum() с новым значением int[]{0, 1}, она суммирует элементы как по строкам,
 * так и по столбцам. В этом двумерном массиве это означает общую сумму элементов внутри!
 * Обратите внимание, что если X - это массив с формой ($2$, $3$) и мы суммируем по столбцам (X.sum(new int[]{0})),
 * то результатом будет (1D) вектор с формой ($3$,). Если мы хотим сохранить количество осей в исходном
 * массиве (в результате получится двумерный массив с формой ($1$, $3$)), вместо того чтобы сворачивать измерение,
 * по которому мы суммировали, мы можем указать true при вызове функции sum().
 */
NDArray X = manager.create(new int[][]{{1, 2, 3}, {4, 5, 6}});
/**
 *Теперь мы готовы к реализации функции softmax. Напомним, что функция softmax состоит из двух этапов:
 * сначала мы возводим в степень каждый член (используя exp()). Затем мы суммируем по каждой строке (у нас есть
 * по одной строке на каждый пример в пакете), чтобы получить константы нормализации для каждого примера.
 * Наконец, мы делим каждую строку на ее константу нормализации, гарантируя, что результат будет равен 1.
 */

System.out.println(X.sum(new int[]{0}, true));
System.out.println(X.sum(new int[]{1}, true));
System.out.println(X.sum(new int[]{0, 1}, true));

    public NDArray softmax(NDArray X) {
        NDArray Xexp = X.exp();
        NDArray partition = Xexp.sum(new int[]{1}, true);
        return Xexp.div(partition); // The broadcast mechanism is applied here
    }

    NDArray X = manager.randomNormal(new Shape(2, 5));
    NDArray Xprob = softmax(X);
System.out.println(Xprob);
System.out.println(Xprob.sum(new int[]{1}));

}
