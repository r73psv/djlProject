package com.github.r73pls.djl_Project.imageClassificftion;

import ai.djl.ndarray.NDArray;
import ai.djl.ndarray.NDList;
import ai.djl.ndarray.NDManager;
import ai.djl.ndarray.index.NDIndex;
import ai.djl.ndarray.types.DataType;
import ai.djl.ndarray.types.Shape;
import lombok.Getter;


public class ModelSoftmax {

    private static NDManager manager = NDManager.newBaseManager();

    /**
     *Каждый пример в необработанных данных представляет собой изображение размером 28*28.
     *  В этом разделе мы сгладим каждое изображение, рассматривая их как 1D-векторы длиной 784.
     *  В регрессии softmax у нас столько выходных данных, сколько существует категорий. Поскольку наш набор данных
     *  содержит 10 категорий, наша сеть будет иметь выходное измерение 10. Следовательно, наши веса будут составлять
     *  матрицу 784*10, а смещения будут составлять вектор 1*10. Мы будем инициализировать наши веса W с помощью
     *  гауссовского шума и наших смещений 0.
     */
    public static NDList params (int numInputs, int numOutputs) {
        NDArray W = manager.randomNormal(0, 0.001f, new Shape(numInputs, numOutputs), DataType.FLOAT32);
        NDArray b = manager.zeros(new Shape(numOutputs), DataType.FLOAT32);
        NDList params = new NDList(W, b);
        return params;
        }

       /**
        * Прежде чем внедрять регрессионную модель softmax, давайте кратко рассмотрим, как такие операторы, как sum(),
        * работают с определенными измерениями в NDArray. Учитывая матрицу X, мы можем суммировать по всем элементам
        * (по умолчанию) или только по элементам на одной оси, т.е. по столбцу (new int[]{0}) или по одной
        * и той же строке (new int[]{1}). Мы помещаем ось в массив int, поскольку мы также можем указать несколько осей.
        * Например, если мы вызываем функцию sum() с новым значением int[]{0, 1}, она суммирует элементы как по строкам,
        * так и по столбцам. В этом двумерном массиве это означает общую сумму элементов внутри!
        * Обратите внимание, что если X - это массив с формой (2, 3) и мы суммируем по столбцам (X.sum(new int[]{0})),
        * то результатом будет (1D) вектор с формой (3,). Если мы хотим сохранить количество осей в исходном массиве
        * (в результате получится двумерный массив с формой (1, 3)), вместо того чтобы сворачивать измерение, по которому
        * мы суммировали, мы можем указать true при вызове функции sum().
        * Напомним, что функция softmax состоит из двух этапов:
        * - сначала мы возводим в степень каждый член (используя exp()).
        * - затем мы суммируем по каждой строке (у нас есть по одной строке на каждый пример в пакете),
        * чтобы получить константы нормализации для каждого примера. Наконец, мы делим каждую строку на ее константу
        * нормализации, гарантируя, что результат будет равен 1.
        *
        *  Для любого случайного ввода мы преобразуем каждый элемент в неотрицательное число.
        * Более того, каждая строка в сумме дает 1, как и требуется для определения вероятности.
        * Обратите внимание, что, хотя математически это выглядит правильно, мы были немного небрежны в нашей реализации,
        * потому что не смогли принять меры предосторожности против числового переполнения или недопотока из-за больших
        * (или очень маленьких) элементов матрицы.
        */
       public static NDArray softmax(NDArray X) {
           NDArray Xexp = X.exp();
           NDArray partition = Xexp.sum(new int[]{1}, true);
           return Xexp.div(partition); // Здесь применяется механизм широковещательной передачи
       }

    /**
     * Теперь, когда мы определили операцию softmax, мы можем реализовать регрессионную модель softmax.
     * Приведенный ниже код определяет прямой проход по сети. Обратите внимание, что мы сглаживаем каждое исходное
     * изображение в вектор с числовыми значениями длины с помощью функции reshape() перед передачей данных через модель.
     */

       public static NDArray net(NDArray X,  int numInputs, int numOutputs) {
            NDList params = params(numInputs, numOutputs);
            NDArray currentW = params.get(0);
            NDArray currentB = params.get(1);
            return softmax(X.reshape(new Shape(-1, numInputs)).dot(currentW).add(currentB));
    }


}
